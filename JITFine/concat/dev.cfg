[file_section]
train_data_file=/home/debian/JIT-Fine/data/jitfine/changes_train.pkl,/home/debian/JIT-Fine/data/jitfine/features_train.pkl
output_dir=/home/debian/JIT-Fine/model/jitfine/saved_models_artificial/checkpoints
eval_data_file=/home/debian/JIT-Fine/data/jitfine/changes_valid.pkl,/home/debian/JIT-Fine/data/jitfine/features_valid.pkl
test_data_file=/home/debian/JIT-Fine/data/jitfine/changes_test.pkl,/home/debian/JIT-Fine/data/jitfine/features_test.pkl

[base_section]
model_name_or_path=microsoft/codebert-base
config_name=microsoft/codebert-base
tokenizer_name=microsoft/codebert-base
cache_dir=


[parameters_section]
train_batch_size=32
eval_batch_size=32
learning_rate=2e-5
max_grad_norm=1.0
seed=42
epochs=50
max_seq_length=512
max_msg_length=64
patience=10
gradient_accumulation_steps=1

[flags_section]
do_train=True
do_eval=False
do_test=True
evaluate_during_training=True